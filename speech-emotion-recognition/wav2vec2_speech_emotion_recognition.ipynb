{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Speech Emotion Recognition using Wav2Vec2**\n",
        "\n",
        "This notebook implements a speech emotion recognition system using\n",
        "transfer learning with a pretrained Wav2Vec2 model.\n",
        "\n",
        "> Note: This project was developed as part of a Deep Learning course assignment.\n"
      ],
      "metadata": {
        "id": "cPcdpR-awFp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Environment Setup***\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8z5r9h4xyMWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import transformers\n",
        "    import safetensors\n",
        "except ImportError:\n",
        "    !pip install -q transformers safetensors\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Reproducibility Configuration\n",
        "SEED = 42\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(SEED)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Compute Device: {device}\")\n",
        "\n",
        "# Configration\n",
        "CONFIG = {\n",
        "    \"SAMPLE_RATE\": 16000,\n",
        "    \"MAX_DURATION\": 4,\n",
        "    \"MODEL_NAME\": \"facebook/wav2vec2-base\",\n",
        "    \"BATCH_SIZE\": 8,\n",
        "    \"BASE_DIR\": \"/content/kaggle_emotion_8actors/kaggle_emotion_8actors\"\n",
        "}\n",
        "\n",
        "# Initialize Processor\n",
        "processor = Wav2Vec2Processor.from_pretrained(CONFIG[\"MODEL_NAME\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKIGerS1yZDQ",
        "outputId": "d6908a1e-d3b9-485d-b2aa-17090802ae78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compute Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Data Loading & Preprocessing Strategy***"
      ],
      "metadata": {
        "id": "GjALlm4WyxAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Emotion labels are mapped to integer classes for training.\n"
      ],
      "metadata": {
        "id": "AM8QcxHWMCO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "AUDIO_DIR = os.path.join(CONFIG[\"BASE_DIR\"], \"audio\")\n",
        "TRAIN_CSV = os.path.join(CONFIG[\"BASE_DIR\"], \"train.csv\")\n",
        "TEST_CSV = os.path.join(CONFIG[\"BASE_DIR\"], \"test.csv\")\n",
        "\n",
        "# Load metadata\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "test_df = pd.read_csv(TEST_CSV)\n",
        "\n",
        "\n",
        "# Standardize Labels\n",
        "label_map = {\n",
        "    \"neutral\": 0, \"calm\": 1, \"happy\": 2, \"sad\": 3,\n",
        "    \"angry\": 4, \"fearful\": 5, \"disgust\": 6, \"surprised\": 7\n",
        "}\n",
        "\n",
        "if train_df[\"label\"].dtype == 'O':\n",
        "    train_df[\"label\"] = train_df[\"label\"].map(label_map)\n",
        "train_df[\"label\"] = train_df[\"label\"].fillna(-1).astype(int)\n",
        "\n",
        "print(f\"Class imbalance fixed. New training size: {len(train_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl_fPsopy9Yu",
        "outputId": "d9dbb141-e030-4e02-9f28-20aba75070f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class imbalance fixed. New training size: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Custom Dataset with Augmentation***"
      ],
      "metadata": {
        "id": "Gn9GG2-y0AFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Wav2VecDataset(Dataset):\n",
        "    def __init__(self, df, audio_dir, augment=False):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.augment = augment\n",
        "        self.target_len = CONFIG[\"SAMPLE_RATE\"] * CONFIG[\"MAX_DURATION\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        path = os.path.join(self.audio_dir, row['Id'])\n",
        "\n",
        "        # Robust Loading\n",
        "        try:\n",
        "            y, sr = librosa.load(path, sr=CONFIG[\"SAMPLE_RATE\"])\n",
        "        except:\n",
        "            y = np.zeros(CONFIG[\"SAMPLE_RATE\"])\n",
        "\n",
        "        # Augmentation\n",
        "        if self.augment:\n",
        "            # 1. Gaussian Noise\n",
        "            if np.random.rand() > 0.5:\n",
        "                noise_amp = 0.005 * np.random.uniform() * np.amax(y)\n",
        "                y = y + noise_amp * np.random.normal(size=y.shape[0])\n",
        "\n",
        "            # 2. Time Shifting\n",
        "            if np.random.rand() > 0.5:\n",
        "                shift_len = int(np.random.uniform(low=-0.1, high=0.1) * CONFIG[\"SAMPLE_RATE\"])\n",
        "                y = np.roll(y, shift_len)\n",
        "\n",
        "        # Pad or Truncate\n",
        "        if len(y) > self.target_len:\n",
        "            y = y[:self.target_len]\n",
        "        else:\n",
        "            y = np.pad(y, (0, self.target_len - len(y)), 'constant')\n",
        "\n",
        "        # Feature Extraction\n",
        "        inputs = processor(\n",
        "            y, sampling_rate=CONFIG[\"SAMPLE_RATE\"], return_tensors=\"pt\", padding=True\n",
        "        )\n",
        "        input_values = inputs.input_values.squeeze(0)\n",
        "\n",
        "        if \"label\" in self.df.columns:\n",
        "            return input_values, torch.tensor(row[\"label\"], dtype=torch.long)\n",
        "        else:\n",
        "            return input_values\n",
        "\n",
        "# Stratified Split\n",
        "train_split, val_split = train_test_split(\n",
        "    train_df, test_size=0.15, random_state=SEED, stratify=train_df['label']\n",
        ")\n",
        "\n",
        "# Addressing Class Imbalance\n",
        "# Neutral has 26 samples, others 52\n",
        "neutral_samples = train_df[train_df['label'] == 'neutral']\n",
        "train_df = pd.concat([train_df, neutral_samples], axis=0).reset_index(drop=True)\n",
        "\n",
        "# Initialize DataLoaders\n",
        "train_loader = DataLoader(Wav2VecDataset(train_split, AUDIO_DIR, augment=True), batch_size=CONFIG[\"BATCH_SIZE\"], shuffle=True)\n",
        "val_loader = DataLoader(Wav2VecDataset(val_split, AUDIO_DIR, augment=False), batch_size=CONFIG[\"BATCH_SIZE\"], shuffle=False)"
      ],
      "metadata": {
        "id": "zVkh1SM80LXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom Dataset and Audio Augmentation**\n",
        "\n",
        "This dataset class handles:\n",
        "- Audio loading and resampling\n",
        "- Padding and truncation\n",
        "- Data augmentation (noise injection, time shifting)\n"
      ],
      "metadata": {
        "id": "TY2zRFGyMKf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Model Architecture & Transfer Learning***"
      ],
      "metadata": {
        "id": "-U4SQBEv1Ogg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading Pre-trained Wav2Vec 2.0...\")\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    CONFIG[\"MODEL_NAME\"],\n",
        "    num_labels=8,\n",
        "    use_safetensors=True\n",
        ").to(device)\n",
        "\n",
        "# Freeze CNN Feature Extractor\n",
        "model.freeze_feature_extractor()\n",
        "\n",
        "# Optimizer & Loss\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5)\n",
        "\n",
        "# Weighted Loss\n",
        "class_weights = torch.tensor([1.0, 1.0, 1.2, 1.0, 1.0, 1.2, 1.0, 1.0]).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HYQCtTZ1NbJ",
        "outputId": "964cd107-a277-460a-cb81-0565cede8101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Pre-trained Wav2Vec 2.0...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Training Loop***"
      ],
      "metadata": {
        "id": "cgFOjqSa17L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30\n",
        "best_acc = 0.0\n",
        "print(\"Starting Validation Training Pipeline...\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_correct = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_correct += (outputs.logits.argmax(1) == labels).sum().item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss += criterion(outputs.logits, labels).item()\n",
        "            val_correct += (outputs.logits.argmax(1) == labels).sum().item()\n",
        "\n",
        "    train_acc = train_correct / len(train_split)\n",
        "    val_acc = val_correct / len(val_split)\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_wav2vec.pth\")\n",
        "\n",
        "print(f\"\\nBest Validation Accuracy: {best_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSaGz1AV2C_g",
        "outputId": "5dd99adb-b346-4dcc-8fae-e8db7a9fc7a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Validation Training Pipeline...\n",
            "Epoch 1/30 | Train Acc: 0.1258 | Val Acc: 0.1379\n",
            "Epoch 2/30 | Train Acc: 0.2331 | Val Acc: 0.2759\n",
            "Epoch 3/30 | Train Acc: 0.2975 | Val Acc: 0.3103\n",
            "Epoch 4/30 | Train Acc: 0.3926 | Val Acc: 0.3448\n",
            "Epoch 5/30 | Train Acc: 0.4908 | Val Acc: 0.4655\n",
            "Epoch 6/30 | Train Acc: 0.5184 | Val Acc: 0.4483\n",
            "Epoch 7/30 | Train Acc: 0.6135 | Val Acc: 0.5345\n",
            "Epoch 8/30 | Train Acc: 0.6810 | Val Acc: 0.5345\n",
            "Epoch 9/30 | Train Acc: 0.7393 | Val Acc: 0.7069\n",
            "Epoch 10/30 | Train Acc: 0.7883 | Val Acc: 0.7759\n",
            "Epoch 11/30 | Train Acc: 0.8589 | Val Acc: 0.8276\n",
            "Epoch 12/30 | Train Acc: 0.8681 | Val Acc: 0.7931\n",
            "Epoch 13/30 | Train Acc: 0.9172 | Val Acc: 0.7586\n",
            "Epoch 14/30 | Train Acc: 0.9387 | Val Acc: 0.8103\n",
            "Epoch 15/30 | Train Acc: 0.9479 | Val Acc: 0.8793\n",
            "Epoch 16/30 | Train Acc: 0.9632 | Val Acc: 0.8448\n",
            "Epoch 17/30 | Train Acc: 0.9264 | Val Acc: 0.8621\n",
            "Epoch 18/30 | Train Acc: 0.9724 | Val Acc: 0.8103\n",
            "Epoch 19/30 | Train Acc: 0.9847 | Val Acc: 0.8276\n",
            "Epoch 20/30 | Train Acc: 0.9908 | Val Acc: 0.8103\n",
            "Epoch 21/30 | Train Acc: 0.9939 | Val Acc: 0.8621\n",
            "Epoch 22/30 | Train Acc: 0.9939 | Val Acc: 0.8966\n",
            "Epoch 23/30 | Train Acc: 0.9969 | Val Acc: 0.8621\n",
            "Epoch 24/30 | Train Acc: 0.9939 | Val Acc: 0.8966\n",
            "Epoch 25/30 | Train Acc: 0.9877 | Val Acc: 0.8103\n",
            "Epoch 26/30 | Train Acc: 0.9908 | Val Acc: 0.8621\n",
            "Epoch 27/30 | Train Acc: 0.9939 | Val Acc: 0.9310\n",
            "Epoch 28/30 | Train Acc: 0.9908 | Val Acc: 0.9310\n",
            "Epoch 29/30 | Train Acc: 0.9969 | Val Acc: 0.9138\n",
            "Epoch 30/30 | Train Acc: 0.9969 | Val Acc: 0.9655\n",
            "\n",
            "Best Validation Accuracy: 0.9655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train–Validation Split and Class Imbalance Handling**\n",
        "\n",
        "A stratified split is used, and class imbalance is mitigated\n",
        "using sample duplication and weighted loss.\n"
      ],
      "metadata": {
        "id": "rlwg1YTKMdXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Evaluation & Inference***"
      ],
      "metadata": {
        "id": "uSkj_w6j2aIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Before deploying the model, I have to verify its performance. I loaded the best weights and generate a Confusion Matrix to inspect class-specific errors*"
      ],
      "metadata": {
        "id": "czwWTFGlCOay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Best Weights\n",
        "model.load_state_dict(torch.load(\"best_wav2vec.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Generate Confusion Matrix\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        y_pred.extend(outputs.logits.argmax(1).cpu().numpy())\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=label_map.keys()))\n",
        "\n",
        "# Generate Submission 1 (Validation Model)\n",
        "test_ds = Wav2VecDataset(test_df, AUDIO_DIR, augment=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=8, shuffle=False)\n",
        "\n",
        "preds_list = []\n",
        "with torch.no_grad():\n",
        "    for inputs in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds_list.extend(outputs.logits.argmax(1).cpu().numpy())\n",
        "\n",
        "inv_label_map = {v: k for k, v in label_map.items()}\n",
        "pred_labels = [inv_label_map[p] for p in preds_list]\n",
        "\n",
        "submission = pd.DataFrame({\"Id\": test_df[\"Id\"], \"label\": pred_labels})\n",
        "submission.to_csv(\"submission_wav2vec_val.csv\", index=False)\n",
        "print(\"submission_wav2vec_val.csv generated successfully!\")"
      ],
      "metadata": {
        "id": "qOlZgFcRBXis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4479f8a-8cab-4f10-dd6b-232a6f032442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral       1.00      0.75      0.86         4\n",
            "        calm       1.00      1.00      1.00         7\n",
            "       happy       1.00      0.86      0.92         7\n",
            "         sad       0.89      1.00      0.94         8\n",
            "       angry       1.00      1.00      1.00         8\n",
            "     fearful       1.00      1.00      1.00         8\n",
            "     disgust       1.00      1.00      1.00         8\n",
            "   surprised       0.89      1.00      0.94         8\n",
            "\n",
            "    accuracy                           0.97        58\n",
            "   macro avg       0.97      0.95      0.96        58\n",
            "weighted avg       0.97      0.97      0.96        58\n",
            "\n",
            "submission_wav2vec_val.csv generated successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Production Training (Full Dataset)**"
      ],
      "metadata": {
        "id": "nlD9SuihDO-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*To achieve maximum performance I perform a Production Run. I combined the Training and Validation sets to utilize 100% of the available data. I initialize the model with the weights from the previous step (best_wav2vec.pth) and fine-tune it for a few epochs at a very low learning rate. This adapts the model to the full data distribution without destroying the learned features*"
      ],
      "metadata": {
        "id": "xHM1DrAGCojk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Starting Production Phase\")\n",
        "full_train_ds = Wav2VecDataset(train_df, AUDIO_DIR, augment=True)\n",
        "production_loader = DataLoader(full_train_ds, batch_size=CONFIG[\"BATCH_SIZE\"], shuffle=True)\n",
        "\n",
        "# Reload Model\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    CONFIG[\"MODEL_NAME\"], num_labels=8, use_safetensors=True\n",
        ").to(device)\n",
        "\n",
        "if os.path.exists(\"best_wav2vec.pth\"):\n",
        "    model.load_state_dict(torch.load(\"best_wav2vec.pth\"))\n",
        "    print(\"Loaded best validation weights. Fine-tuning...\")\n",
        "\n",
        "model.freeze_feature_extractor()\n",
        "\n",
        "# Lower Learning Rate for Fine-Tuning\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=1e-2)\n",
        "ACCUM_STEPS = 4\n",
        "\n",
        "print(\"Starting Production Training (10 Epochs)...\")\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(production_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.logits, labels) / ACCUM_STEPS\n",
        "        loss.backward()\n",
        "\n",
        "        if (i + 1) % ACCUM_STEPS == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item() * ACCUM_STEPS\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Prod Loss: {running_loss/len(production_loader):.4f}\")\n",
        "\n",
        "# Final Submission Generation\n",
        "model.eval()\n",
        "preds_list = []\n",
        "with torch.no_grad():\n",
        "    for inputs in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds_list.extend(outputs.logits.argmax(1).cpu().numpy())\n",
        "\n",
        "final_labels = [inv_label_map[p] for p in preds_list]\n",
        "submission_final = pd.DataFrame({\"Id\": test_df[\"Id\"], \"label\": final_labels})\n",
        "submission_final.to_csv(\"submission_production_final.csv\", index=False)\n",
        "print(\"submission_production_final.csv generated!\")"
      ],
      "metadata": {
        "id": "40LdwL-IDJsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "533fddb5-42eb-49f5-bd5c-1d7f51364fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Starting Production Phase\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded best validation weights. Fine-tuning...\n",
            "Starting Production Training (10 Epochs)...\n",
            "Epoch 1 | Prod Loss: 0.1683\n",
            "Epoch 2 | Prod Loss: 0.1447\n",
            "Epoch 3 | Prod Loss: 0.1300\n",
            "Epoch 4 | Prod Loss: 0.1275\n",
            "Epoch 5 | Prod Loss: 0.1192\n",
            "Epoch 6 | Prod Loss: 0.1235\n",
            "Epoch 7 | Prod Loss: 0.1086\n",
            "Epoch 8 | Prod Loss: 0.1317\n",
            "Epoch 9 | Prod Loss: 0.1010\n",
            "Epoch 10 | Prod Loss: 0.0930\n",
            "submission_production_final.csv generated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***References***\n",
        "\n",
        "\n",
        "\n",
        "[1] A. Baevski et al., “wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,”\n",
        "in Advances in Neural Information Processing Systems (NeurIPS), 2020.  \n",
        "https://arxiv.org/abs/2006.11477\n",
        "\n",
        "[2] Livingstone, S. R., & Russo, F. A., “The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS),”\n",
        "PLOS ONE, 2018.  \n",
        "https://doi.org/10.1371/journal.pone.0196391\n",
        "\n",
        "[3] Kaggle Dataset: *Speech Emotion Recognition (RAVDESS)*.  \n",
        "Available at: https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio\n",
        "\n",
        "[4] Hugging Face Transformers Documentation.  \n",
        "https://huggingface.co/docs/transformers/model_doc/wav2vec2\n"
      ],
      "metadata": {
        "id": "lXogwGLq_lB6"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}